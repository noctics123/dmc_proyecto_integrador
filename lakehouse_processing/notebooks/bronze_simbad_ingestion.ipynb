{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMBAD Landing ‚Üí Bronze Pipeline\n",
    "\n",
    "**Prop√≥sito**: Convierte datos CSV de SIMBAD desde landing a formato Parquet en bronze layer\n",
    "\n",
    "**Funcionalidades**:\n",
    "- Detecta autom√°ticamente el √∫ltimo directorio dt=YYYY-MM-DD\n",
    "- Manejo robusto de encoding (UTF-8 fallback a ISO-8859-1)\n",
    "- Esquema estable con tipos de datos correctos\n",
    "- Particionado inteligente por a√±o/mes\n",
    "- Trazabilidad completa (archivo origen, dt_captura)\n",
    "\n",
    "**Input**: `gs://bucket/lakehouse/landing/simbad/simbad_carteras_aayp_hipotecarios/dt=*/`\n",
    "**Output**: `gs://bucket/lakehouse/bronze/simbad/simbad_carteras_aayp_hipotecarios/anio=*/mes=*/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# SIMBAD landing -> bronze (schema estable)\n",
    "# ==============================================\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "BUCKET = \"dae-integrador-2025\"\n",
    "LANDING = f\"gs://{BUCKET}/lakehouse/landing/simbad/simbad_carteras_aayp_hipotecarios\"\n",
    "BRONZE_BASE = f\"gs://{BUCKET}/lakehouse/bronze/simbad/simbad_carteras_aayp_hipotecarios\"\n",
    "\n",
    "print(f\"üîÑ Iniciando SIMBAD Landing ‚Üí Bronze\")\n",
    "print(f\"üì• Source: {LANDING}\")\n",
    "print(f\"üì§ Target: {BRONZE_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- utilidades para dt ---\n",
    "def _list_subdirs(gcs_dir: str):\n",
    "    \"\"\"Lista subdirectorios usando Hadoop FileSystem\"\"\"\n",
    "    jsc = sc._jsc\n",
    "    hconf = jsc.hadoopConfiguration()\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    fs = FileSystem.get(Path(gcs_dir).toUri(), hconf)\n",
    "    return [st.getPath().getName() for st in fs.listStatus(Path(gcs_dir)) if st.isDirectory()]\n",
    "\n",
    "def _pick_latest_dt_dir(base_dir: str):\n",
    "    \"\"\"Encuentra el directorio dt= m√°s reciente\"\"\"\n",
    "    subdirs = _list_subdirs(base_dir)\n",
    "    dt_dirs = [d for d in subdirs if d.startswith(\"dt=\")]\n",
    "    if not dt_dirs:\n",
    "        raise RuntimeError(f\"No se encontraron directorios dt= en {base_dir}\")\n",
    "    latest = max(dt_dirs, key=lambda d: datetime.strptime(d.split(\"=\")[1], \"%Y-%m-%d\"))\n",
    "    return latest, latest.split(\"=\")[1]\n",
    "\n",
    "# --- lee CSV del mayor dt (con fallback de encoding) ---\n",
    "def _read_csv(path_glob: str):\n",
    "    \"\"\"Lee CSV con manejo robusto de encoding\"\"\"\n",
    "    try:\n",
    "        df = (spark.read\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"mode\", \"PERMISSIVE\")\n",
    "              .option(\"multiLine\", \"false\")\n",
    "              .option(\"inferSchema\", \"false\")     # todo como STRING\n",
    "              .option(\"encoding\", \"UTF-8\")\n",
    "              .csv(path_glob))\n",
    "        print(\"‚úÖ Lectura exitosa con UTF-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è UTF-8 fall√≥, probando ISO-8859-1: {str(e)}\")\n",
    "        df = (spark.read\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"mode\", \"PERMISSIVE\")\n",
    "              .option(\"multiLine\", \"false\")\n",
    "              .option(\"inferSchema\", \"false\")\n",
    "              .option(\"encoding\", \"ISO-8859-1\")\n",
    "              .csv(path_glob))\n",
    "        print(\"‚úÖ Lectura exitosa con ISO-8859-1\")\n",
    "    return df.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- columnas esperadas (en camelCase) ---\n",
    "EXPECTED = [\n",
    "    \"periodo\",\"tipoCredito\",\"tipoEntidad\",\"entidad\",\"sectorEconomico\",\"region\",\"provincia\",\n",
    "    \"moneda\",\"tipoCartera\",\"actividad\",\"sector\",\"persona\",\"facilidad\",\"residencia\",\n",
    "    \"administracionYPropiedad\",\"genero\",\"tipoCliente\",\"clasificacionEntidad\",\n",
    "    \"cantidadPlasticos\",\"cantidadCredito\",\"deuda\",\"tasaPorDeuda\",\"deudaCapital\",\n",
    "    \"deudaVencida\",\"deudaVencidaDe31A90Dias\",\"valorDesembolso\",\"valorGarantia\",\n",
    "    \"valorProvisionCapitalYRendimiento\",\"__periodo\"\n",
    "]\n",
    "\n",
    "# --- ingest ---\n",
    "dt_dir, dt_str = _pick_latest_dt_dir(LANDING)\n",
    "csv_glob = f\"{LANDING}/{dt_dir}/*.csv\"\n",
    "print(f\"üìÅ √öltimo directorio encontrado: {dt_dir}\")\n",
    "print(f\"üìÑ Leyendo: {csv_glob}\")\n",
    "\n",
    "df_raw = _read_csv(csv_glob)\n",
    "print(f\"üìä Filas raw: {df_raw.count():,}\")\n",
    "print(f\"üìã Columnas raw: {len(df_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegura presencia/orden de columnas\n",
    "sel = [ (F.col(c) if c in df_raw.columns else F.lit(None).alias(c)) for c in EXPECTED ]\n",
    "df = df_raw.select(*sel)\n",
    "\n",
    "print(\"‚úÖ Esquema normalizado a columnas esperadas\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza y TIPOS FIJOS (un solo select)\n",
    "df_cast = df.select(\n",
    "    # Campos string (con trim)\n",
    "    F.trim(F.col(\"periodo\")).alias(\"periodo\"),\n",
    "    F.trim(F.col(\"tipoCredito\")).alias(\"tipoCredito\"),\n",
    "    F.trim(F.col(\"tipoEntidad\")).alias(\"tipoEntidad\"),\n",
    "    F.trim(F.col(\"entidad\")).alias(\"entidad\"),\n",
    "    F.trim(F.col(\"sectorEconomico\")).alias(\"sectorEconomico\"),\n",
    "    F.trim(F.col(\"region\")).alias(\"region\"),\n",
    "    F.trim(F.col(\"provincia\")).alias(\"provincia\"),\n",
    "    F.trim(F.col(\"moneda\")).alias(\"moneda\"),\n",
    "    F.trim(F.col(\"tipoCartera\")).alias(\"tipoCartera\"),\n",
    "    F.trim(F.col(\"actividad\")).alias(\"actividad\"),\n",
    "    F.trim(F.col(\"sector\")).alias(\"sector\"),\n",
    "    F.trim(F.col(\"persona\")).alias(\"persona\"),\n",
    "    F.trim(F.col(\"facilidad\")).alias(\"facilidad\"),\n",
    "    F.trim(F.col(\"residencia\")).alias(\"residencia\"),\n",
    "    F.trim(F.col(\"administracionYPropiedad\")).alias(\"administracionYPropiedad\"),\n",
    "    F.trim(F.col(\"genero\")).alias(\"genero\"),\n",
    "    F.trim(F.col(\"tipoCliente\")).alias(\"tipoCliente\"),\n",
    "    F.trim(F.col(\"clasificacionEntidad\")).alias(\"clasificacionEntidad\"),\n",
    "\n",
    "    # Campos num√©ricos\n",
    "    F.col(\"cantidadPlasticos\").cast(\"int\").alias(\"cantidadPlasticos\"),\n",
    "    F.col(\"cantidadCredito\").cast(\"int\").alias(\"cantidadCredito\"),\n",
    "    F.col(\"deuda\").cast(\"double\").alias(\"deuda\"),\n",
    "    F.col(\"tasaPorDeuda\").cast(\"double\").alias(\"tasaPorDeuda\"),\n",
    "    F.col(\"deudaCapital\").cast(\"double\").alias(\"deudaCapital\"),\n",
    "    F.col(\"deudaVencida\").cast(\"double\").alias(\"deudaVencida\"),\n",
    "    F.col(\"deudaVencidaDe31A90Dias\").cast(\"double\").alias(\"deudaVencidaDe31A90Dias\"),\n",
    "    F.col(\"valorDesembolso\").cast(\"double\").alias(\"valorDesembolso\"),\n",
    "    F.col(\"valorGarantia\").cast(\"double\").alias(\"valorGarantia\"),\n",
    "    F.col(\"valorProvisionCapitalYRendimiento\").cast(\"double\").alias(\"valorProvisionCapitalYRendimiento\"),\n",
    "\n",
    "    F.col(\"__periodo\").alias(\"__periodo\"),\n",
    "\n",
    "    # derivados y trazabilidad\n",
    "    F.to_date(F.col(\"periodo\"), \"yyyy-MM\").alias(\"periodo_date\"),\n",
    "    F.year(F.to_date(F.col(\"periodo\"), \"yyyy-MM\")).alias(\"anio\"),\n",
    "    F.month(F.to_date(F.col(\"periodo\"), \"yyyy-MM\")).alias(\"mes\"),\n",
    "    F.lit(dt_str).alias(\"dt_captura\"),\n",
    "    F.input_file_name().alias(\"archivo_origen\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tipos de datos aplicados y columnas derivadas creadas\")\n",
    "print(f\"üìä Filas procesadas: {df_cast.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra sample de datos\n",
    "print(\"üìã Sample de datos procesados:\")\n",
    "df_cast.select(\"periodo\", \"tipoEntidad\", \"entidad\", \"deuda\", \"anio\", \"mes\", \"dt_captura\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- escritura por a√±o (carpeta) y partici√≥n por mes ---\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "anios = [r.anio for r in df_cast.select(\"anio\").distinct().collect()]\n",
    "anios_sorted = sorted(anios)\n",
    "\n",
    "print(f\"üìÖ A√±os a procesar: {anios_sorted}\")\n",
    "\n",
    "for y in anios_sorted:\n",
    "    out = f\"{BRONZE_BASE}/anio={y}\"\n",
    "    filas_anio = df_cast.filter(F.col(\"anio\")==y).count()\n",
    "    \n",
    "    print(f\"‚è≥ Escribiendo a√±o {y} ({filas_anio:,} filas) ‚Üí {out}\")\n",
    "    \n",
    "    (df_cast.filter(F.col(\"anio\")==y)\n",
    "            .write.mode(\"overwrite\")\n",
    "            .partitionBy(\"mes\")\n",
    "            .parquet(out))\n",
    "    \n",
    "    print(f\"‚úÖ {y} completado\")\n",
    "\n",
    "print(\"üéâ ¬°SIMBAD Bronze ingestion completada exitosamente!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}