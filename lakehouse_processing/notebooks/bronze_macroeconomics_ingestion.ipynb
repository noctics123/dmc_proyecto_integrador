{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macroeconomics Landing ‚Üí Bronze Pipeline\n",
    "\n",
    "**Prop√≥sito**: Convierte datos CSV de indicadores macroecon√≥micos desde landing a formato Parquet en bronze layer\n",
    "\n",
    "**Datasets**:\n",
    "- Desempleo (IMF data)\n",
    "- Inflaci√≥n 12M \n",
    "- Tipo de cambio\n",
    "\n",
    "**Funcionalidades**:\n",
    "- Procesamiento autom√°tico de m√∫ltiples datasets\n",
    "- Esquemas espec√≠ficos por tipo de dato\n",
    "- Particionado por fecha de proceso\n",
    "- Manejo de encoding UTF-8\n",
    "\n",
    "**Input**: `gs://bucket/lakehouse/landing/macroeconomics/*/dt=*/`\n",
    "**Output**: `gs://bucket/lakehouse/bronze/macroeconomics/bronze_*_data/dt=*/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Macroeconomics landing -> bronze\n",
    "# ==============================================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "BUCKET = \"dae-integrador-2025\"\n",
    "LANDING_BASE = f\"gs://{BUCKET}/lakehouse/landing/macroeconomics\"\n",
    "BRONZE_BASE = f\"gs://{BUCKET}/lakehouse/bronze/macroeconomics\"\n",
    "\n",
    "print(f\"üîÑ Iniciando Macroeconomics Landing ‚Üí Bronze\")\n",
    "print(f\"üì• Source: {LANDING_BASE}\")\n",
    "print(f\"üì§ Target: {BRONZE_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades\n",
    "def _list_subdirs(gcs_dir: str):\n",
    "    \"\"\"Lista subdirectorios usando Hadoop FileSystem\"\"\"\n",
    "    jsc = sc._jsc\n",
    "    hconf = jsc.hadoopConfiguration()\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    fs = FileSystem.get(Path(gcs_dir).toUri(), hconf)\n",
    "    return [st.getPath().getName() for st in fs.listStatus(Path(gcs_dir)) if st.isDirectory()]\n",
    "\n",
    "def _pick_latest_dt_dir(base_dir: str):\n",
    "    \"\"\"Encuentra el directorio dt= m√°s reciente\"\"\"\n",
    "    subdirs = _list_subdirs(base_dir)\n",
    "    dt_dirs = [d for d in subdirs if d.startswith(\"dt=\")]\n",
    "    if not dt_dirs:\n",
    "        raise RuntimeError(f\"No se encontraron directorios dt= en {base_dir}\")\n",
    "    latest = max(dt_dirs, key=lambda d: datetime.strptime(d.split(\"=\")[1], \"%Y-%m-%d\"))\n",
    "    return latest, latest.split(\"=\")[1]\n",
    "\n",
    "def process_dataset(dataset_name: str, schema: StructType):\n",
    "    \"\"\"Procesa un dataset espec√≠fico\"\"\"\n",
    "    print(f\"\\nüìä Procesando dataset: {dataset_name}\")\n",
    "    \n",
    "    # Buscar √∫ltimo dt\n",
    "    landing_path = f\"{LANDING_BASE}/{dataset_name}\"\n",
    "    try:\n",
    "        dt_dir, dt_str = _pick_latest_dt_dir(landing_path)\n",
    "        csv_glob = f\"{landing_path}/{dt_dir}/*.csv\"\n",
    "        print(f\"üìÅ √öltimo dt: {dt_dir}\")\n",
    "        print(f\"üìÑ Leyendo: {csv_glob}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è No se encontraron datos para {dataset_name}: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Leer CSV\n",
    "    try:\n",
    "        df_raw = (spark.read\n",
    "                 .option(\"header\", \"true\")\n",
    "                 .option(\"delimiter\", \",\")\n",
    "                 .option(\"encoding\", \"UTF-8\")\n",
    "                 .schema(schema)\n",
    "                 .csv(csv_glob))\n",
    "        \n",
    "        if df_raw.rdd.isEmpty():\n",
    "            print(f\"‚ö†Ô∏è Dataset {dataset_name} est√° vac√≠o\")\n",
    "            return\n",
    "            \n",
    "        row_count = df_raw.count()\n",
    "        print(f\"üìä Filas le√≠das: {row_count:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error leyendo {dataset_name}: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # A√±adir metadatos\n",
    "    df_bronze = (df_raw\n",
    "                .withColumn(\"fecha_proceso\", F.current_date())\n",
    "                .withColumn(\"dt_captura\", F.lit(dt_str))\n",
    "                .withColumn(\"archivo_origen\", F.input_file_name()))\n",
    "    \n",
    "    # Escribir a Bronze\n",
    "    bronze_path = f\"{BRONZE_BASE}/bronze_{dataset_name}_data\"\n",
    "    \n",
    "    try:\n",
    "        (df_bronze.write\n",
    "         .mode(\"overwrite\")\n",
    "         .format(\"parquet\")\n",
    "         .partitionBy(\"dt_captura\")\n",
    "         .save(bronze_path))\n",
    "        \n",
    "        print(f\"‚úÖ {dataset_name} escrito en: {bronze_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error escribiendo {dataset_name}: {str(e)}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esquemas por dataset\n",
    "schemas = {\n",
    "    \"desempleo_imf\": StructType([\n",
    "        StructType().add(\"fecha\", StringType(), True)\n",
    "                   .add(\"pais\", StringType(), True)\n",
    "                   .add(\"tasa_desempleo\", StringType(), True)\n",
    "                   .add(\"fuente\", StringType(), True)\n",
    "    ]),\n",
    "    \n",
    "    \"inflacion_12m\": StructType([\n",
    "        StructType().add(\"fecha\", StringType(), True)\n",
    "                   .add(\"pais\", StringType(), True)\n",
    "                   .add(\"inflacion_anual\", StringType(), True)\n",
    "                   .add(\"indice_base\", StringType(), True)\n",
    "    ]),\n",
    "    \n",
    "    \"tipo_cambio\": StructType([\n",
    "        StructType().add(\"fecha\", StringType(), True)\n",
    "                   .add(\"moneda_origen\", StringType(), True)\n",
    "                   .add(\"moneda_destino\", StringType(), True)\n",
    "                   .add(\"tasa_cambio\", StringType(), True)\n",
    "                   .add(\"tipo_cambio\", StringType(), True)\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"üìã Esquemas definidos para datasets:\")\n",
    "for dataset in schemas.keys():\n",
    "    print(f\"  - {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar todos los datasets\n",
    "print(\"üöÄ Iniciando procesamiento de datasets...\")\n",
    "\n",
    "for dataset_name, schema in schemas.items():\n",
    "    process_dataset(dataset_name, schema)\n",
    "\n",
    "print(\"\\nüéâ ¬°Macroeconomics Bronze ingestion completada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificaci√≥n final\n",
    "print(\"\\nüìã Verificaci√≥n de datos en Bronze:\")\n",
    "\n",
    "for dataset_name in schemas.keys():\n",
    "    bronze_path = f\"{BRONZE_BASE}/bronze_{dataset_name}_data\"\n",
    "    try:\n",
    "        df_check = spark.read.parquet(bronze_path)\n",
    "        count = df_check.count()\n",
    "        print(f\"  ‚úÖ {dataset_name}: {count:,} filas\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {dataset_name}: Error - {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}