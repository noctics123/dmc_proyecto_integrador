# Lakehouse Processing Pipeline

Pipeline de procesamiento de datos para arquitectura lakehouse con DataProc y PySpark.

## ğŸ“ Estructura

```
lakehouse_processing/
â”œâ”€â”€ ğŸ“ notebooks/                   # Notebooks de DataProc (Jupyter)
â”‚   â”œâ”€â”€ bronze_simbad_ingestion.ipynb          # Landing â†’ Bronze SIMBAD
â”‚   â”œâ”€â”€ bronze_macroeconomics_ingestion.ipynb  # Landing â†’ Bronze MacroeconomÃ­a
â”‚   â”œâ”€â”€ silver_data_cleaning.ipynb             # Bronze â†’ Silver (limpieza)
â”‚   â””â”€â”€ gold_metrics_aggregation.ipynb         # Silver â†’ Gold (mÃ©tricas)
â”‚
â””â”€â”€ README.md                       # DocumentaciÃ³n del pipeline DataProc
```

## ğŸ¯ Capas del Lakehouse

### **Landing Layer**
- **Formato**: CSV raw data
- **Fuentes**: SIMBAD API, PowerBI MacroeconomÃ­a
- **Estructura**: `gs://bucket/lakehouse/landing/{source}/dt=YYYY-MM-DD/`

### **Bronze Layer**
- **Formato**: Parquet (sin transformaciones)
- **PropÃ³sito**: Ingesta raw con esquema estable
- **Particionado**: Por aÃ±o/mes (SIMBAD), por dt_captura (Macro)
- **Estructura**: `gs://bucket/lakehouse/bronze/{source}/anio=*/mes=*/`

### **Silver Layer**
- **Formato**: Parquet (datos limpios)
- **PropÃ³sito**: DeduplicaciÃ³n, normalizaciÃ³n, validaciÃ³n
- **Estructura**: `gs://bucket/lakehouse/silver/{source}/`

### **Gold Layer**
- **Formato**: Parquet (mÃ©tricas de negocio)
- **PropÃ³sito**: KPIs, agregaciones, mÃ©tricas finales
- **Estructura**: `gs://bucket/lakehouse/gold/{metric_type}/`

## ğŸš€ Uso en DataProc

### ConfiguraciÃ³n del Cluster
```bash
# Cluster actual configurado con:
- Jupyter habilitado
- ZEPPELIN, TRINO, DELTA disponibles
- UbicaciÃ³n: us-central1
- Nombre: cluster-integrador-2025
```

### Ejecutar Notebooks
1. **Acceder a Jupyter**:
   - Consola GCP â†’ DataProc â†’ Clusters â†’ Web Interfaces
   - Seleccionar Jupyter

2. **UbicaciÃ³n de Notebooks**:
   - Los notebooks estÃ¡n en esta carpeta del repositorio
   - Copiar a DataProc o ejecutar directamente

3. **Orden de EjecuciÃ³n**:
   ```
   1. bronze_simbad_ingestion.ipynb      # Si hay datos SIMBAD nuevos
   2. bronze_macroeconomics_ingestion.ipynb  # Para datos macro
   3. silver_data_cleaning.ipynb         # Limpieza de datos
   4. gold_metrics_aggregation.ipynb     # MÃ©tricas finales
   ```

## ğŸ“Š Datasets Procesados

### SIMBAD (Superintendencia de Bancos RD)
- **Fuente**: `landing/simbad/simbad_carteras_aayp_hipotecarios/`
- **Contenido**: CrÃ©ditos hipotecarios AAyP (2012-presente)
- **Volumen**: ~676K filas por carga completa
- **Frecuencia**: Incremental (mensual via landing)

### MacroeconomÃ­a
- **Fuente**: `landing/macroeconomics/`
- **Datasets**:
  - `desempleo_imf` - Datos de desempleo FMI
  - `inflacion_12m` - InflaciÃ³n anualizada
  - `tipo_cambio` - Tasas de cambio
- **Frecuencia**: Diaria (via PowerBI scraper)

## âš™ï¸ ConfiguraciÃ³n

### Variables Principales (config.py)
```python
BUCKET = "dae-integrador-2025"
REGION = "us-west2"
CLUSTER = "cluster-integrador-2025"
```

### Esquemas y Particionado
- **SIMBAD**: Particionado por `anio`/`mes` basado en campo `periodo`
- **MacroeconomÃ­a**: Particionado por `dt_captura` (fecha de carga)

## ğŸ”§ Desarrollo

### AÃ±adir Nuevo Dataset
1. Actualizar `config.py` con nueva configuraciÃ³n
2. Crear notebook especÃ­fico en `notebooks/`
3. Definir esquema y particionado apropiado
4. Implementar validaciones de calidad

### Debugging
- Logs disponibles en DataProc cluster
- Verificar datos en cada capa antes de procesar siguiente
- Usar `.count()` y `.show()` para validar transformaciones

## ğŸ“ˆ Monitoreo

### MÃ©tricas Clave
- **Filas procesadas** por capa
- **Tiempo de ejecuciÃ³n** por notebook
- **Calidad de datos** (nulls, duplicados)
- **Cobertura temporal** (perÃ­odos procesados)

### Validaciones
- Verificar particiones creadas correctamente
- Confirmar esquemas consistentes entre capas
- Validar rangos de fechas y valores numÃ©ricos