# Estrategia de Carga Incremental Optimizada

## ðŸŽ¯ **Estrategia Recomendada: HÃ­brida**

### **Carga HistÃ³rica (Una vez)**
- **DataProc Notebooks**: Para carga inicial completa (2012-presente)
- **Formato**: CSV â†’ Parquet (optimizado para consultas histÃ³ricas)
- **Uso**: Solo cuando se requiere reprocesar todo el histÃ³rico

### **Carga Incremental (Diaria/AutomÃ¡tica)**
- **BigQuery Direct**: Lee CSV landing directo a Silver
- **Ventaja**: Sin dependencia de DataProc cluster activo
- **Eficiencia**: Solo procesa nuevos dt= automÃ¡ticamente

## ðŸ“‹ **ComparaciÃ³n de Enfoques**

| Aspecto | DataProc Incremental | BigQuery Direct CSV | HÃ­brida (Recomendada) |
|---------|---------------------|-------------------|----------------------|
| **Costo** | Alto (cluster activo) | Bajo (solo query) | Ã“ptimo |
| **Latencia** | Media (espera cluster) | Baja (inmediata) | Baja |
| **Complejidad** | Alta (PySpark + BQ) | Media (SQL puro) | Media |
| **Dependencias** | DataProc + BigQuery | Solo BigQuery | MÃ­nimas |
| **Escalabilidad** | Buena | Excelente | Excelente |

## ðŸ—ï¸ **Arquitectura Optimizada**

```mermaid
graph LR
    A[SIMBAD API] --> B[Landing CSV]
    B --> C{Tipo Carga}
    C -->|HistÃ³rica| D[DataProc Notebooks]
    C -->|Incremental| E[BigQuery Direct]
    D --> F[Bronze Parquet]
    F --> G[BigQuery External Table]
    E --> H[Silver Tables]
    G --> H
    H --> I[Gold Metrics]
```

## ðŸ”„ **Flujos de Procesamiento**

### **Flujo HistÃ³rico (Inicial)**
1. **Landing**: CSV histÃ³rico completo en GCS
2. **DataProc**: Notebooks procesan CSV â†’ Parquet Bronze
3. **BigQuery**: External tables leen Parquet Bronze
4. **Silver**: TransformaciÃ³n Bronze â†’ Silver
5. **Gold**: Agregaciones Silver â†’ Gold

### **Flujo Incremental (Diario)**
1. **Landing**: Nuevo CSV con dt=YYYY-MM-DD
2. **BigQuery**: SP detecta nuevo dt= automÃ¡ticamente
3. **Silver**: Procesa CSV landing directo a Silver
4. **Gold**: Actualiza solo mÃ©tricas afectadas

## ðŸ“ **Stored Procedures Creados**

### **Para Carga Incremental Optimizada:**

1. **`sp_process_landing_to_silver_incremental`**
   - Detecta nuevos dt= en landing CSV
   - Procesa solo archivos nuevos
   - Limpieza y validaciÃ³n robusta
   - DeduplicaciÃ³n automÃ¡tica

2. **`sp_process_silver_to_gold`**
   - Actualiza mÃ©tricas incrementalmente
   - Solo procesa perÃ­odos afectados
   - Enriquece con datos macroeconÃ³micos

3. **`sp_full_pipeline_refresh`**
   - Orquestador principal
   - Manejo de errores centralizado
   - Logging completo

## âš¡ **Optimizaciones Clave**

### **External Table CSV Inteligente**
```sql
CREATE EXTERNAL TABLE simbad_landing_csv_ext
OPTIONS (
  format = 'CSV',
  uris = ['gs://bucket/landing/simbad/*/dt=*/*.csv']
)
```
- **Wildcards**: Detecta nuevos archivos automÃ¡ticamente
- **Path parsing**: Extrae dt_captura del _FILE_NAME
- **Flexible**: Maneja mÃºltiples formatos de fecha

### **DetecciÃ³n Incremental AutomÃ¡tica**
```sql
-- Solo procesar archivos mÃ¡s nuevos que Ãºltimo dt_captura en Silver
WHERE REGEXP_EXTRACT(_FILE_NAME, r'/dt=(\d{4}-\d{2}-\d{2})/') >
  (SELECT MAX(dt_captura) FROM silver_clean.simbad_hipotecarios)
```

### **Parsing Robusto de Datos**
```sql
-- Limpieza numÃ©rica tolerante a caracteres especiales
SAFE_CAST(REGEXP_REPLACE(REGEXP_REPLACE(deudaCapital, r'[,$]', ''), r'[^0-9.-]', '') AS FLOAT64)

-- Parsing de fechas con mÃºltiples formatos
COALESCE(
  SAFE.PARSE_DATE('%Y-%m', periodo),
  SAFE.PARSE_DATE('%Y/%m', periodo),
  SAFE.PARSE_DATE('%m/%Y', periodo)
)
```

## ðŸ“Š **Monitoreo y Alertas**

### **MÃ©tricas de Control**
- **Filas procesadas** por dt_captura
- **Tiempo de ejecuciÃ³n** por stored procedure
- **Calidad de datos** (flags de error)
- **Cobertura temporal** (gaps en fechas)

### **Logs Centralizados**
```sql
SELECT process_name, status, rows_processed, message
FROM gold.process_log
WHERE created_at >= CURRENT_DATE()
ORDER BY created_at DESC
```

## ðŸš€ **Scheduling Recomendado**

### **Cloud Scheduler Jobs**
1. **Incremental SIMBAD**: Diario 6:00 AM
   ```bash
   gcloud scheduler jobs create http simbad-incremental \
     --schedule="0 6 * * *" \
     --uri="https://us-central1-proyecto-integrador-dae-2025.cloudfunctions.net/trigger-bq-incremental"
   ```

2. **Gold Metrics Update**: Diario 7:00 AM
   ```bash
   gcloud scheduler jobs create http gold-metrics-update \
     --schedule="0 7 * * *" \
     --uri="https://us-central1-proyecto-integrador-dae-2025.cloudfunctions.net/trigger-gold-update"
   ```

## ðŸŽ¯ **Beneficios de la Estrategia HÃ­brida**

### **Costos Optimizados**
- âœ… DataProc: Solo para carga histÃ³rica (ocasional)
- âœ… BigQuery: Processing incremental eficiente
- âœ… Storage: CSV + Parquet segÃºn necesidad

### **Performance Mejorado**
- âœ… Latencia baja para datos nuevos
- âœ… Sin overhead de cluster DataProc
- âœ… Queries BigQuery optimizadas

### **Mantenimiento Simplificado**
- âœ… Menos dependencias de infraestructura
- âœ… SQL puro para lÃ³gica incremental
- âœ… Logging y monitoreo centralizado

### **Escalabilidad**
- âœ… BigQuery escala automÃ¡ticamente
- âœ… External tables detectan nuevos archivos
- âœ… Processing paralelo natural

## ðŸ“‹ **PrÃ³ximos Pasos**

1. **Crear external table** para CSV landing
2. **Deploy stored procedures** en BigQuery
3. **Configurar Cloud Scheduler** para automatizaciÃ³n
4. **Implementar monitoring** con alertas
5. **Deprecar notebooks** de DataProc para incremental