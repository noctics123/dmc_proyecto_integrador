# Arquitectura DMC - Pipeline de Datos Integral

## ğŸ—ï¸ **VisiÃ³n General**

Pipeline de datos completo para anÃ¡lisis de crÃ©ditos hipotecarios SIMBAD e indicadores macroeconÃ³micos usando arquitectura lakehouse hÃ­brida en Google Cloud Platform.

## ğŸ“Š **Arquitectura por Capas**

```mermaid
graph TD
    A[SIMBAD API] --> B[Landing Layer - CSV]
    C[PowerBI MacroeconomÃ­a] --> B
    B --> D{Tipo de Carga}
    D -->|HistÃ³rica| E[DataProc - Notebooks]
    D -->|Incremental| F[BigQuery Direct]
    E --> G[Bronze Layer - Parquet]
    G --> H[BigQuery External Tables]
    F --> I[Silver Layer - BigQuery]
    H --> I
    I --> J[Gold Layer - MÃ©tricas]
    J --> K[Looker Studio]
    J --> L[Alertas]

    style B fill:#e1f5fe
    style G fill:#f3e5f5
    style I fill:#e8f5e8
    style J fill:#fff3e0
```

## ğŸ”„ **Servicios GCP por Capa**

### **ğŸŒŠ Landing Layer**
| Servicio | Uso | ConfiguraciÃ³n |
|----------|-----|---------------|
| **Cloud Storage** | Almacenamiento CSV raw | `gs://dae-integrador-2025/lakehouse/landing/` |
| **Cloud Run** | SIMBAD API harvester | Trigger automÃ¡tico por HTTP |
| **Cloud Scheduler** | AutomatizaciÃ³n de cargas | Diario 5:00 AM |
| **Cloud Build** | CI/CD para deployers | Trigger en push a main |

**Estructura de Datos:**
```
landing/
â”œâ”€â”€ simbad/simbad_carteras_aayp_hipotecarios/dt=2024-09-14/*.csv
â””â”€â”€ macroeconomics/
    â”œâ”€â”€ desempleo_imf/dt=2024-09-14/*.csv
    â”œâ”€â”€ inflacion_12m/dt=2024-09-14/*.csv
    â””â”€â”€ tipo_cambio/dt=2024-09-14/*.csv
```

### **ğŸ”§ Bronze Layer**
| Servicio | Uso | ConfiguraciÃ³n |
|----------|-----|---------------|
| **DataProc** | Notebooks Jupyter para carga histÃ³rica | cluster-integrador-2025 |
| **Cloud Storage** | Parquet optimizado | `gs://dae-integrador-2025/lakehouse/bronze/` |
| **BigQuery** | External tables sobre Parquet | Auto-detecciÃ³n con wildcards |

**Responsabilidades:**
- âœ… **DataProc**: Carga histÃ³rica completa (2012-presente)
- âœ… **PySpark**: ConversiÃ³n CSV â†’ Parquet con esquemas estables
- âœ… **Particionado**: SIMBAD por anio/mes, Macro por dt_captura

**Notebooks DataProc:**
```
lakehouse_processing/notebooks/
â”œâ”€â”€ bronze_simbad_ingestion.ipynb          # 676K registros histÃ³ricos
â”œâ”€â”€ bronze_macroeconomics_ingestion.ipynb  # Datos macro por dataset
â”œâ”€â”€ silver_data_cleaning.ipynb             # Placeholder
â””â”€â”€ gold_metrics_aggregation.ipynb         # Placeholder
```

### **ğŸ§¼ Silver Layer**
| Servicio | Uso | ConfiguraciÃ³n |
|----------|-----|---------------|
| **BigQuery** | Procesamiento y almacenamiento | Tablas clustered |
| **BigQuery Stored Procedures** | LÃ³gica de transformaciÃ³n | Prefijo `sp_` |
| **Cloud Scheduler** | Trigger automÃ¡tico | Diario 6:00 AM |

**Transformaciones:**
- ğŸ”„ **Incremental**: Solo nuevos dt_captura desde landing CSV
- ğŸ§¹ **Limpieza**: DeduplicaciÃ³n, normalizaciÃ³n, validaciÃ³n
- ğŸ“Š **Enriquecimiento**: Flags de calidad, campos derivados
- ğŸ—ï¸ **Clustering**: Por entidad y tipoCliente

**Stored Procedures Silver:**
```sql
-- Incremental optimizado (recomendado)
CALL sp_process_landing_to_silver_incremental();

-- Desde bronze parquet (fallback)
CALL sp_process_bronze_to_silver();
```

### **ğŸ† Gold Layer**
| Servicio | Uso | ConfiguraciÃ³n |
|----------|-----|---------------|
| **BigQuery** | MÃ©tricas agregadas | Particionado temporal |
| **BigQuery Stored Procedures** | KPIs y agregaciones | Automatizado |
| **Cloud Scheduler** | Refresh mÃ©tricas | Diario 7:00 AM |

**MÃ©tricas de Negocio:**
- ğŸ“ˆ **KPIs Financieros**: Tasa mora, cobertura garantÃ­a, provisiones
- ğŸŒ **Agregaciones**: Por provincia, entidad, perÃ­odo
- ğŸ“Š **MacroeconomÃ­a**: InflaciÃ³n, tipo cambio integrado
- âš ï¸ **Alertas**: DetecciÃ³n automÃ¡tica de riesgos

**Tablas Gold:**
```sql
-- MÃ©tricas principales
gold.simbad_gold                    -- Agregado por provincia/perÃ­odo
gold.alertas_provincia_12m          -- Sistema de alertas
gold.alertas_provincia_12m_looker   -- Vista optimizada para Looker
```

### **ğŸ“Š Presentation Layer**
| Servicio | Uso | ConfiguraciÃ³n |
|----------|-----|---------------|
| **Looker Studio** | Dashboards ejecutivos | Conectado a BigQuery |
| **BigQuery** | API para consultas | Acceso directo |
| **Cloud Functions** | Alertas automÃ¡ticas | Trigger por cambios |

## ğŸš€ **CI/CD Pipeline**

### **Cloud Build Triggers**
| Trigger | Fuente | AcciÃ³n |
|---------|--------|--------|
| **main-deploy** | Push a `main` | Deploy completo |
| **desarrollo-test** | Push a `desarrollo` | Testing |
| **simbad-harvester** | Cambios en `landing/` | Deploy harvester |

### **AutomatizaciÃ³n**
```yaml
# .github/workflows or cloudbuild.yaml
steps:
  - name: 'Deploy Landing Services'
    env: ['PROJECT=proyecto-integrador-dae-2025']
  - name: 'Update BigQuery Procedures'
    env: ['DATASET=silver_clean']
  - name: 'Test Pipeline End-to-End'
    env: ['RUN_VALIDATION=true']
```

## â° **Scheduling y Triggers**

### **Cloud Scheduler Jobs**
| Job | Horario | Servicio Target | FunciÃ³n |
|-----|---------|-----------------|---------|
| `simbad-daily-harvest` | 5:00 AM | Cloud Run | Extraer datos SIMBAD |
| `macro-daily-harvest` | 5:30 AM | Cloud Run | Extraer datos macro |
| `silver-incremental` | 6:00 AM | BigQuery | Procesar nuevos datos |
| `gold-metrics-refresh` | 7:00 AM | BigQuery | Actualizar mÃ©tricas |
| `alertas-daily-check` | 8:00 AM | Cloud Function | Enviar alertas |

### **Event-Driven Triggers**
- **Cloud Storage**: Trigger automÃ¡tico al llegar nuevos CSV
- **BigQuery**: Trigger en completaciÃ³n de silver para gold
- **Pub/Sub**: Notificaciones entre servicios

## ğŸ’¾ **Almacenamiento y OptimizaciÃ³n**

### **Cloud Storage**
```
Bucket: dae-integrador-2025
â”œâ”€â”€ /lakehouse/landing/     # CSV raw (30 dÃ­as retention)
â”œâ”€â”€ /lakehouse/bronze/      # Parquet optimizado (1 aÃ±o)
â””â”€â”€ /logs/                  # Logs de procesamiento
```

### **BigQuery**
```
Proyecto: proyecto-integrador-dae-2025
â”œâ”€â”€ bronze/                 # External tables
â”œâ”€â”€ silver_clean/          # Tablas clustered
â”œâ”€â”€ gold/                  # MÃ©tricas particionadas
â””â”€â”€ logs/                  # Process logs
```

**Optimizaciones:**
- âœ… **Particionado temporal** en gold por perÃ­odo
- âœ… **Clustering** en silver por entidad
- âœ… **External tables** para reducir costos de storage
- âœ… **Incremental loading** para eficiencia

## ğŸ” **Seguridad y Governance**

### **IAM Roles**
- **DataProc**: `roles/dataproc.worker`
- **BigQuery**: `roles/bigquery.dataEditor`
- **Cloud Storage**: `roles/storage.objectAdmin`
- **Cloud Scheduler**: `roles/cloudscheduler.admin`

### **Data Lineage**
```
SIMBAD API â†’ Landing CSV â†’ {Bronze Parquet | Direct Silver} â†’ Gold Metrics â†’ Looker
PowerBI â†’ Landing CSV â†’ Bronze Parquet â†’ Gold Metrics â†’ Alertas
```

## ğŸ“ˆ **Monitoreo y Alertas**

### **MÃ©tricas Clave**
- **Latencia**: Tiempo landing â†’ gold < 2 horas
- **Volumen**: 676K+ registros SIMBAD procesados
- **Calidad**: < 1% registros con flags de error
- **Disponibilidad**: 99.9% uptime del pipeline

### **Logging Centralizado**
```sql
-- Verificar estado del pipeline
SELECT process_name, status, rows_processed, message
FROM gold.process_log
WHERE created_at >= CURRENT_DATE()
ORDER BY created_at DESC
```

## ğŸ¯ **Roadmap TÃ©cnico**

### **Optimizaciones Actuales**
- âœ… Arquitectura hÃ­brida DataProc + BigQuery
- âœ… Carga incremental automatizada
- âœ… Stored procedures con manejo de errores
- âœ… External tables para reducir costos

### **PrÃ³ximas Mejoras**
- ğŸ”„ **Real-time streaming** con Pub/Sub + Dataflow
- ğŸ¤– **ML Pipeline** para predicciÃ³n de mora
- ğŸ“Š **Data Quality** automatizado con Great Expectations
- ğŸŒ **Multi-region** para alta disponibilidad

---

## ğŸ› ï¸ **Quick Start**

### **Deploy Pipeline Completo**
```bash
# 1. Deploy servicios de landing
gcloud builds submit --config=landing/cloudbuild.yaml

# 2. Crear external tables en BigQuery
bq query < bigquery_processing/schemas/bronze_external_tables.sql

# 3. Deploy stored procedures
bq query < bigquery_processing/stored_procedures/sp_*.sql

# 4. Configurar scheduling
gcloud scheduler jobs create http simbad-daily --schedule="0 5 * * *"
```

### **Ejecutar Pipeline Manual**
```sql
-- Pipeline completo incremental
CALL `proyecto-integrador-dae-2025.gold.sp_full_pipeline_refresh`();

-- Solo silver incremental
CALL `proyecto-integrador-dae-2025.bronze.sp_process_landing_to_silver_incremental`();
```